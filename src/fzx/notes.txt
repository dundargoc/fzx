Try to reuse already calculated results. If the query was changed in a way that can only
narrow down the results, reuse them. And if new items have been added, don't discard
previous results. It's going to be a bit more complicated than it sounds, because each
thread has its own range of items to process, which changes according to the total item
count.

------------------------------------------------------------------------------------------

Cache results. Narrowing the results goes in one direction, but there is nothing to handle
going in the other direction. If the results were fully calculated, cache the first 128
indices (sums up to 1kb per cached query, which is pretty good), and store it in a hash
map or a fixed size queue that automatically invalidates older results. If that query is
given again, send those back and kick off the matching again in the background. If items
beyond those 128 are requested for the UI to display, return those as unknown (null or
something) and tell the UI to come back for actual strings later. So the UI would have to
display them as unknown too, if you scroll beyond the cached 128 items, which isn't ideal
but 90% of the time you're not going to do that. I don't think there is any better way,
none of the other options are particularly good if you have the requirement of never
blocking the UI.

I considered caching all results, but this only matters if there is a lot of items, so
that will also take some space to store it. If the items weren't sorted I could compact
the integers into an array of u8, u16 and u32, but they can be reordered, so that won't
work, and even if you had variable sized integers (kinda like UTF-8), you'd lose a fast
lookup by index anyway. Now that I think about it, maybe caching all results can be
combined into two levels of cache, with separate space limits. But it doesn't actually
solve the problems of the cache from the paragraph above, and it would rarely be useful,
so why bother.

------------------------------------------------------------------------------------------

Store the size of the string inside the strings buffer. Right now the limit on a single
item size is UINT32_MAX (only if you have a single item). Lower that to UINT16_MAX (max
64kb per item) so it takes up just two bytes in memory. The max amount of items still
stays at UINT32_MAX. What I hope this enables is that if we're just iterating over the
items from start to end, we won't need to first load the {offset,length} pairs that's
stored elsewhere to get to the item. Having {offset,length} is still necessary, but it
then will be used only for getting results, or skipping to the right location when
dividing the work into chunks for different threads.

------------------------------------------------------------------------------------------

On linux, use mmap for allocating items. With mmap an existing allocation can be expanded
in size, so you don't need to copy elements to the new memory. I doubt it will have any
impact on performace, but that will keep down memory usage. I don't know in detail how
virtual memory works, so the question here is if by doing so the memory gets fragmented in
physical RAM and what are the implications of that. Also maybe try to be smart about it,
so if doubling the memory size fails, try to allocate less memory? Is there some way of
knowing how much can you allocate at a specific address without having to parse the /proc
stuff?

------------------------------------------------------------------------------------------

Do the benchmarks properly. They are awful and can't really be relied on for anything
other than just getting the general idea, which was good enough at the beginning, but now
they stopped being useful. First of all the benchmarks that use multithreading include the
overhead from creating and joining threads, which doesn't happen in the actual library.
Second, use real input data. For testing and benchmarking the entire worker pool, EventFd
should also provide a wait function, so we don't have to go through the OS to receive
results.

------------------------------------------------------------------------------------------

Work stealing. Measure the time each thread spends on processing items and see if the work
is distributed fairly and if not, does it even matter at all. It's possible that all the
items are ordered, the work is divided in equal ranges, but the query will only match the
items for one thread, making it do all the heavy work on scoring that everyone has to wait
for, while all other threads are doing nothing.

------------------------------------------------------------------------------------------

Compare against the stuff from <algorithm> and <execution>.

------------------------------------------------------------------------------------------

Vectorize sorting. Sorting takes up very roughly 5% of the runtime, so it's not worth
spending any time on it right now. But in the case it goes up because of the improvements
on the matching, it could make sense to try to somehow vectorize that. What needs to be
sorted is the {u32,f32} pairs, together. Should be doable with pshufb on intel, and I have
no idea if there is any equivalent instruction on neon. But realistically speaking, I'm
probably not smart enough to actually do this.

------------------------------------------------------------------------------------------

Windows support. EventFd class probably needs Windows specific code. Also __builtin_*
functions are used in WorkerPool, which isn't going to work on MSVC. For that maybe just
make a precomputed lookup table to define a dependency tree between the threads.
