Minimal requirements for the first release:
[X] Bounds checking in Fzx::pushItem. String buffer and item count can't overflow uint32_t
[ ] WorkerPool exception safety. std::thread calls std::terminate on unhandled exceptions

------------------------------------------------------------------------------------------

Try to reuse already calculated results. If the query was changed in a way that can only
narrow down the results, reuse them. And if new items have been added, don't discard
previous results.

------------------------------------------------------------------------------------------

Cache results. Narrowing the results goes in one direction, but there is nothing to handle
going in the other direction. If the results were fully calculated, cache the first 128
indices (sums up to 1kb per cached query, which is pretty good), and store it in a hash
map or a fixed size queue that automatically invalidates older results. If that query is
given again, send those back and kick off the matching again in the background. If items
beyond those 128 are requested for the UI to display, return those as unknown (null or
something) and tell the UI to come back for actual strings later. So the UI would have to
display them as unknown too, if you scroll beyond the cached 128 items, which isn't ideal
but 90% of the time you're not going to do that. I don't think there is any better way,
none of the other options are particularly good if you have the requirement of never
blocking the UI.

I considered caching all results, but this only matters if there is a lot of items, so
that will also take some space to store it. If the items weren't sorted I could compact
the integers into an array of u8, u16 and u32, but they can be reordered, so that won't
work, and even if you had variable sized integers (kinda like UTF-8), you'd lose a fast
lookup by index anyway. Now that I think about it, maybe caching all results can be
combined into two levels of cache, with separate space limits. But it doesn't actually
solve the problems of the cache from the paragraph above, and it would rarely be useful,
so why bother.

------------------------------------------------------------------------------------------

Store the size of the string inside the strings buffer. Right now the limit on a single
item size is UINT32_MAX (only if you have a single item). Lower that to UINT16_MAX (max
64kb per item) so it takes up just two bytes in memory. The max amount of items still
stays at UINT32_MAX. What I hope this enables is that if we're just iterating over the
items from start to end, we won't need to first load the {offset,length} pairs that's
stored elsewhere to get to the item. Having {offset,length} is still necessary, but it
then will be used only for getting results, or skipping to the right location when
dividing the work into chunks for different threads.

------------------------------------------------------------------------------------------

On linux, use mmap for allocating items. With mmap an existing allocation can be expanded
in size, so you don't need to copy elements to the new memory. I doubt it will have any
impact on performace, but that will keep down memory usage. I don't know in detail how
virtual memory works, so the question here is if by doing so the memory gets fragmented in
physical RAM and what are the implications of that. Also maybe try to be smart about it,
so if doubling the memory size fails, try to allocate less memory? Is there some way of
knowing how much can you allocate at a specific address without having to parse the /proc
stuff?

------------------------------------------------------------------------------------------

Vectorize sorting. I can't remember exactly, but I think sorting can take about 10% of
the runtime? Anyway, to do that matches will probably have to be stored in AoSoA, because
items are represented as pairs of {f32,u32}. In this case both sorting and merging have to
be vectorized, can't have one without the other.
